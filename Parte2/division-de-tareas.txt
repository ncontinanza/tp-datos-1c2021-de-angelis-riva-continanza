<<========================== Modelos sí o sí =========================>>

Siempre tener mínimo un 10% para test **OBLIGATORIO**

* Árbol: (Limitar la altura del árbol y limitar la cantidad mínima de casos en las hojas)
(OK)Lukas   - Preprocessing de la primera parte.
(OK)Lukas   - Dejar 8000 personas con alto poder adquisitivo y 8000 personas con bajo (Seguramente da mal, pero hay que probar todo)

* SVM:
(OK)Nico    - Preprocessing significantes (PCA con las primeras 30 mejores componentes)
(OK)Lukas   - Preprocessing mejor_tsne (buscar las mejores features que nos den algún patrón en tsne)
Nico        - PCA + TSNE (Realizar PCA y buscar el mejor N que separa los datos en TSNE antes de aplicar SVM)

* KNN:
(OK)Lukas   - Preprocessing significantes (PCA con las primeras N mejores componentes) [Buscar el mejor N]
(OK)Nico        - Preprocessing mejor_tsne (buscar las mejores features que nos den algún patrón en tsne)
Nico        - PCA + TSNE (Realizar PCA y buscar el mejor N que separa los datos en TSNE antes de aplicar SVM)

* Bayes (Escalado de todas las variables positivas (Usar minmax_scale entre 0 y 1))
(OK)Nico    - Preprocessing de la primera parte.
(OK)Lukas   - Distintos tipos de variables

* Logistic Regression (Escalar los dato estandarizando (sklearn.preprocessing.scale) y dummy variables, no missings)
(OK)Nico    - Sólo variables numéricas.
(OK)Nico    - Preprocessing de la primera parte haciendo dummy variables.
(OK)Lukas   - Preprocessing mediante árbol (Agarrar las primeras N raíces de un árbol de decisión con todas las variables)

* AdaBoost
(OK)Nico    - Crudo (No mandar NaNs) [No hace falta escalar ni hacer one hot encoding] 
(OK)Nico   - Preprocessing significantes.

* XGBoost (Según internet es similar a AdaBoost)
(Falta correrlo) Nico    - Crudo (No mandar NaNs) [No hace falta escalar ni hacer one hot encoding] 
Lukas   - Preprocessing significantes.

* Random forest:
(OK)Lukas   - Preprocessing de la primera parte.
(OK)Lukas   - Dejar 8000 personas con alto poder adquisitivo y 8000 personas con bajo (Seguramente da mal, pero hay que probar todo)
(OK)Lukas   - Todas las variables (pero recortar mucho la altura)

* Bagging (Árbol):
(OK)Lukas   - Preprocessing de la primera parte.
(OK)Lukas   - Todas las variables, con límite de altura.

* Bagging (KNN):
    ?       - Sólo las variables numéricas.
    ?       - PCA con 70% de la varianza (O 90%, a ver)

<<========================== Si nos da =========================>>


* Voting (Sobre todos los modelos (con sus mejores preprocessings) que voten todos (que sea impar))

* Cascading (Ir buscando el método con mayor precisión [Ver si para 0 o para 1, lo que mejor venga] e ir entrenando modelos con las variables que van quedando (las que no fueron predichas por no tener tal probabilidad. (La probabilidad la estimamos con predic_proba))

* Red neuronal:

<<========================== Detalles =========================>>

* Tener en cuenta los NaN en variables que no tenían missings en primera instancia (por ejemplo edad) ver de utilizar algún imputer (por ejemplo KNN imputer, etc)

